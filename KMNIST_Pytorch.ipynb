{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPHQEIu3NBFl"
   },
   "outputs": [],
   "source": [
    "# Jovian Commit Essentials\n",
    "# Please retain and execute this cell without modifying the contents for `jovian.commit` to work\n",
    "!pip install jovian --upgrade -q\n",
    "import jovian\n",
    "jovian.set_project('project-course-gkomliki')\n",
    "jovian.set_colab_id('1CxJCQ6eMGn43Kg3YkSLiulc6-eCrrw7N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKj6Vqu6YmM3"
   },
   "outputs": [],
   "source": [
    "# Project name used for jovian.commit\n",
    "project_name = 'Project_course_gkomliki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "qztwDQyqZINy",
    "outputId": "8739226b-d494-4163-9228-5921dff3973b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/gkomliki/project-course-gkomliki\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/gkomliki/project-course-gkomliki'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project_name,environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vdtfCkqmJVe"
   },
   "source": [
    "In this project we will work the KMNIST dataset which is already incorporated in the `torchvision.datasets` library. This specific dataset Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.The first step will be to import some important libraries that they will proven useful in the training of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdrvvNUxZQQ8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import transforms as tt\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EudAr1DfbZaV"
   },
   "source": [
    "## 1.**Downloading the dataset**\n",
    "The first thing we'll do is to download our datset straight from the `torchviviondatasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s9RNLClcsGq"
   },
   "outputs": [],
   "source": [
    "dataset= KMNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8fNmBlfc2c0",
    "outputId": "db214951-fb34-43ea-c13d-2be68db9d7f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the length of the dataset\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnSs9Vu3QRo6",
    "outputId": "ee2f3ff9-b462-45df-d84e-ab7863ed722d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7FBD6C56E6D0>, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCPce30dnQc9"
   },
   "source": [
    "The image is an object of the class `PIL.Image.Image`, which is a part of the Python imaging library Pillow. We can view the image using matplotlib or plotly, the plotting and graphing libraries\n",
    " for data science in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zv_t6A-KgRZL",
    "outputId": "29188cb8-39c1-4ccc-89b0-5cfc21898eea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking of the test_dataset lengh\n",
    "test_dataset = KMNIST(root='data/',train=False)\n",
    "len(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoLvzn_Qgd2t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "38FBJdbQo7nn",
    "outputId": "6e8fdf59-60c1-4f64-c9e5-cd90e9108cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0rd-qigpV4V"
   },
   "source": [
    "We transform the PIL image into Tensor using  `ToTensor()` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8xNUKdukJtO"
   },
   "outputs": [],
   "source": [
    "dataset = KMNIST(root='data/',train=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXmOvpt_Rpvc",
    "outputId": "26884f49-4226-458e-ec5e-d4e2d904c56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 8\n"
     ]
    }
   ],
   "source": [
    "img,label=dataset[0]\n",
    "print(img.shape,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTZOXUHwL6OK"
   },
   "source": [
    "Let's visualize our tensot this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Lzc4XmxKSmrU",
    "outputId": "14572794-9e00-4367-b865-3ed3644d9890"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[0,15:20,15:20],cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CnFqM1fk4dN"
   },
   "source": [
    "## 2. **Splitting our dataset** \n",
    "The next step is to split the dataset into training and validation set in order ot train our model and later to validate its predictions (chekc if the predictions are closely approximate the labels). We define a validation set size=10000 and the rest of the data we'll be used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3jOTEwLkzdN",
    "outputId": "0da5e202-1180-4e03-bc9e-1c159c46e536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = 10000\n",
    "train_size= len(dataset)- valid_size\n",
    "# Splitting of the data using the random_split\n",
    "train_ds, valid_ds = random_split(dataset, [train_size,valid_size])\n",
    "len(train_ds),len(valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "U0pALYL-n45u",
    "outputId": "62cae8da-2a64-43b4-e603-c9c1eb135c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/gkomliki/project-course-gkomliki\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/gkomliki/project-course-gkomliki'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbcjcBxNqATy"
   },
   "source": [
    "## 3. **Data Loading usinf DataLoaders**\n",
    "Next step will be to load our data in batces to the dataloaders. We define a batch size of 128 and if necessary we will adapt it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnAYeDbpzd_K"
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoAp34rU14HG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egitZJHK18xx"
   },
   "outputs": [],
   "source": [
    "# Loading the data using DataLoader()\n",
    "train_dl=DataLoader (train_ds, batch_size, shuffle=True,num_workers=2,pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, 2*batch_size, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "ZKD6qKKu2qLz",
    "outputId": "de51ee04-465a-40d6-d060-ffca20bf2fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/gkomliki/project-course-gkomliki\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/gkomliki/project-course-gkomliki'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project_name,Environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gr7qvrXucl67"
   },
   "source": [
    "## 4. **Data Transfer form CPU to GPU**\n",
    "\n",
    "The next step is to create a pipeline that will move data form a CPU to the colab GPU. GPU can optimize the performance of the matrix operations and significantly increase the speed of the training of our model. In the selected dataset, the use of a GPU will not be necessary. However we construct the pipeline to indicate how this process could have taken place of necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVFjoghxdrdh"
   },
   "source": [
    "First we chack if the required NVIDIA cuba drivers are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPewaheodzcF",
    "outputId": "5db9955c-9107-46bd-b3bc-8eeb4a5ac5d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdsXH4zDeFoU"
   },
   "source": [
    "Now, we procced by defining a helper function to ensure that our code uses the GPU if it is available and defaults to using the CPU if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omnIIBa9eNdF"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "  # Use GPU if available else use CPU\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfuTWPbfeshM",
    "outputId": "e048ec13-f697-4a3d-d041-7a62cf5f7064"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKDEU44kfmEy"
   },
   "source": [
    "Next, we define our pipeline for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQfe9VyEfjYe"
   },
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "  # Move Tensors data to the chosen device\n",
    "  if isinstance(data,(list,tuple)):\n",
    "    return [to_device(x,device) for x in data]\n",
    "  return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLd-NgJKg10c",
    "outputId": "071001d2-5901-47e6-9f8a-41770b6561df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for img,labels in train_dl:\n",
    "  print(img.shape)\n",
    "  images= to_device(img,device)\n",
    "  print(images.device)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bob7mawlhhrY"
   },
   "source": [
    "The last step is to define a `DeviceDataLoader` class to wrap our existing data loaders and move batches of data to the selected device. We don't need to extend an existing class to create a PyTorch data loader. What we need is an `__iter__` method to retrieve batches of data and an `__len__` method to get the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hFf8-zjhQXZ"
   },
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "  # Wrap a data loader to move data to a device\n",
    "  def __init__(self, dl, device):\n",
    "    self.dl = dl\n",
    "    self.device = device\n",
    "  \n",
    "  def __iter__(self):\n",
    "    # Yield a batch of data after moving it to the device\n",
    "    for x in self.dl:\n",
    "      yield to_device(x,self.device)\n",
    "\n",
    "  def __len__(self):\n",
    "    # Number of batches\n",
    "    return len(self.dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYNkky5CjxDg"
   },
   "source": [
    "We can now wrap our data loaders using `DeviceDataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWNJdh_jjq2O"
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl,device)\n",
    "valid_dl = DeviceDataLoader(valid_dl,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pd4SKOikig1"
   },
   "source": [
    "Tensors moved to the GPU have a `device` property which includes the word cuda or cpu. Let's verify this by looking at a batch of data from train_dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3XO51BnkcDa",
    "outputId": "b4511bd9-a640-4e83-f638-4f315b1ba3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.device: cuda:0\n",
      "yb: tensor([4, 9, 0, 6, 3, 2, 6, 7, 4, 7, 6, 3, 4, 0, 4, 7, 3, 5, 6, 9, 0, 5, 7, 8,\n",
      "        6, 8, 5, 5, 1, 8, 1, 3, 1, 8, 0, 4, 7, 1, 1, 6, 4, 1, 4, 6, 7, 5, 9, 8,\n",
      "        2, 3, 1, 3, 3, 7, 4, 6, 6, 5, 8, 4, 9, 8, 6, 8, 1, 9, 4, 3, 9, 9, 9, 1,\n",
      "        7, 2, 4, 2, 4, 2, 1, 9, 0, 3, 5, 3, 8, 5, 3, 9, 9, 4, 6, 0, 5, 1, 1, 5,\n",
      "        6, 2, 4, 0, 4, 1, 6, 3, 5, 1, 2, 6, 3, 5, 8, 3, 2, 3, 1, 5, 9, 8, 8, 3,\n",
      "        5, 5, 1, 6, 7, 9, 9, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print('xb.device:', xb.device)\n",
    "    print('yb:', yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "UskjXlh9rm43",
    "outputId": "21bf1474-4907-4626-957d-59478fb57d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/gkomliki/project-course-gkomliki\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/gkomliki/project-course-gkomliki'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project_name,Environment=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nim94a1kl2bS"
   },
   "source": [
    "## 5. ***Defining and Training the model***:\n",
    "\n",
    "For the purpose of the project we will start by defining a ***Convolutional Neural Network (CNN)***, train it, and based on its calculated accuracy we will decide if a further action is required e.g. adding a residual block architecture for instance. Let's start by defining the model:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui04qM3xk0hE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXFcIka6ZG8Y"
   },
   "outputs": [],
   "source": [
    "input_size=28*28\n",
    "num_classes=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4f9M9CupNaO"
   },
   "source": [
    "We define a class which will help us for train and validation of our model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTK9ZHP80dqN"
   },
   "source": [
    "We'll use `nn.Sequential` to chain the  CNN layers and activations functions into a single network architecture defined below arbitrally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTAkCJBM0osT"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    \n",
    "class KMnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), #output 64x14x14\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(64*14*14, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10))\n",
    "            #self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        #xb = xb.reshape(-1, 784)\n",
    "        #out = self.linear(xb)\n",
    "        return self.network(xb)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = KMnistModel()    \n",
    "   \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbDKfXoMF9y2"
   },
   "outputs": [],
   "source": [
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwWNgYp8FBYD",
    "outputId": "2e88ac53-8973-48a1-d69f-ff4b82c8833e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 1, 28, 28])\n",
      "out.shape: torch.Size([128, 10])\n",
      "out[0]: tensor([ 0.1165, -0.0021,  0.0029, -0.0999,  0.0852,  0.0223, -0.1458,  0.0100,\n",
      "        -0.0669, -0.0469], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    print('images.shape:', images.shape)\n",
    "    out = model(images)\n",
    "    print('out.shape:', out.shape)\n",
    "    print('out[0]:', out[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7LvOx2Z75nJ"
   },
   "source": [
    "Let's transfer batches of data to the GPU , and use to_device to move our model to the GPU (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Co7fq-GY7l_B"
   },
   "outputs": [],
   "source": [
    "to_device(model, device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "EMOJ6jAF8Yld",
    "outputId": "47c07b36-0750-427d-a9d0-a1dc8d882e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Detected Colab notebook...\u001b[0m\n",
      "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
      "Committed successfully! https://jovian.ai/gkomliki/project-course-gkomliki\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://jovian.ai/gkomliki/project-course-gkomliki'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEm72P0c98gp"
   },
   "source": [
    "It is time to train our model. We define 2 functions: `fit` and `evaluate` \n",
    "and we use gradient descent ti evaluate the model performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLQHmLk393zT"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,valid_dl):\n",
    "  model.eval()\n",
    "  out = [model.validation_step(batch) for batch in valid_dl]\n",
    "  return model.validation_epoch_end(out)\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Em9OQlEFAjI4"
   },
   "source": [
    "Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvuBtJdPAivH"
   },
   "outputs": [],
   "source": [
    "model = to_device(KMnistModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mL0EfEt8Lm4n",
    "outputId": "d890638c-6498-4e69-8678-a72c48d445fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3046212196350098, 'val_acc': 0.11513672024011612}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKQnLM7uApEA"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHLDPalaBoxe",
    "outputId": "628d9a41-6f79-4dcb-a6f6-e5cb7cdabb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Hyperparams logged.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.reset()\n",
    "jovian.log_hyperparams({\n",
    "    'num_epochs': num_epochs,\n",
    "    'opt_func': opt_func.__name__,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMQlPuGFFp_B",
    "outputId": "b62bb147-7213-4311-e01d-4b0893c74ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.4062, val_acc: 0.8675\n",
      "Epoch [1], val_loss: 0.2790, val_acc: 0.9098\n",
      "Epoch [2], val_loss: 0.2191, val_acc: 0.9325\n",
      "Epoch [3], val_loss: 0.1947, val_acc: 0.9394\n",
      "Epoch [4], val_loss: 0.2004, val_acc: 0.9389\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMd0TkrAune1"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEyEZkMmusDF",
    "outputId": "0bf0a0dc-fa68-4fb4-eaf9-deb8a1b9a20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.1413, val_acc: 0.9584\n",
      "Epoch [1], val_loss: 0.1391, val_acc: 0.9585\n",
      "Epoch [2], val_loss: 0.1367, val_acc: 0.9600\n",
      "Epoch [3], val_loss: 0.1371, val_acc: 0.9615\n",
      "Epoch [4], val_loss: 0.1340, val_acc: 0.9627\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgTmwKsfvjSA"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOTANOr0wEek",
    "outputId": "8c3e5e7c-5bfb-4744-e83a-296bf45b7381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Hyperparams logged.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.reset()\n",
    "jovian.log_hyperparams({\n",
    "    'num_epochs': num_epochs,\n",
    "    'opt_func': opt_func.__name__,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDcF71Gnvmv6",
    "outputId": "c02955ab-4e8a-4275-8741-bb6527bd0f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.1317, val_acc: 0.9624\n",
      "Epoch [1], val_loss: 0.1320, val_acc: 0.9620\n",
      "Epoch [2], val_loss: 0.1322, val_acc: 0.9627\n",
      "Epoch [3], val_loss: 0.1322, val_acc: 0.9625\n",
      "Epoch [4], val_loss: 0.1322, val_acc: 0.9628\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGZkZ3LPys-E"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpP1HBnyv2U"
   },
   "outputs": [],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKNMUgMevy7f"
   },
   "source": [
    "The model performs quite well with almost 96% accuracy. Perhaps with the introduction of a residual block or a `resnet` architecture we could possibly achive accuracies close to 99%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1b2tKHbwGdh"
   },
   "source": [
    "We can also plot the valdation set accuracies to study how the model improves over time. We define a plot_accuracies function to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CuoxeUSwTD3"
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFUAJ6IUwaHw"
   },
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHg4XWS7wkip"
   },
   "source": [
    "Our model reaches an accuracy of around 96%, and by looking at the graph, it seems unlikely that the model will achieve an accuracy higher than that even after training for a long time. \n",
    "We can also plot the training and validation losses to study the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTkXh3yows5H"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy6niwDtwxj5"
   },
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnZAyW_ozA3n"
   },
   "source": [
    "Initialy, both the training and validation losses seem to decrease over time. However, if you train the model for long enough, you will notice that the training loss continues to decrease, while the validation loss stops decreasing, and even starts to increase after a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_4sSVVXzH7c"
   },
   "source": [
    "## 6. **Test With individual images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D6cIwIFzQM6"
   },
   "outputs": [],
   "source": [
    "# Define test dataset\n",
    "test_dataset = KMNIST(root='data/', \n",
    "                     train=False,\n",
    "                     transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ0S7SuIzcjJ"
   },
   "source": [
    "Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3OgfWhFzZGS"
   },
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBo61WnlznRn"
   },
   "source": [
    "Let's try to predict some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ry2G1Cbzmj_"
   },
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-bAFV9d1qC-"
   },
   "outputs": [],
   "source": [
    "img, label = test_dataset[7233]\n",
    "\n",
    "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZgrS7_51-jc"
   },
   "source": [
    "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
    "\n",
    "As a final step, let's also look at the overall loss and accuracy of the model on the test set, and record using `jovian`. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDNy9fo-13fj"
   },
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxsDywqM2C5I"
   },
   "outputs": [],
   "source": [
    "jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8n6b1eJK922"
   },
   "outputs": [],
   "source": [
    "jovian.commit(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clVYFl522Ic9"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The FashionMNIST dataset is a well-known dataset used for image classifications problems. We have a defined an arbitrary deep learning architecture which gave adequately good resultsand can be used as a reference to fusther improve the model in the future with the use of Residual blocks architectures perhaps or by addinf more Convolutional layers to capture more details in the images and produce better results. The model was trained at GPU cuba drivers in a computatioanly efficient way. For the future I will focus on trying different architectures in more complicated datasets and expand my skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2SBVbVPJ0zY"
   },
   "source": [
    "## References:\n",
    "1) https://github.com/rois-codh/kmnist: The KMNIST dataset\n",
    "\n",
    "2) https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53: A smooth introduction to Convolutional Neural networks.\n",
    "\n",
    "3) https://towardsdatascience.com/how-to-traine-tensorflow-models-79426dabd304 : How to use a GPU to train a machine learning model.\n",
    "\n",
    "4) https://pytorch.org/ : A complete guide to Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
